# ClawOS Architecture

## Core Concept: Trust-Aware Data Flow

Every piece of data in an agent system has a source. The fundamental insight is:
**if you track where data came from, you can make intelligent decisions about what it's allowed to do.**

Traditional security tries to detect "bad" content. ClawOS takes a different approach:
all content is treated according to its provenance, not its pattern.

## Data Flow

```
User message ──→ [Tag: source=user, trust=trusted]
                    │
                    ▼
              ┌──────────┐
              │   Agent   │ ──→ Tool call decision
              └──────────┘
                    │
                    ▼
Tool output ──→ [Tag: source=tool:web_search, trust=untrusted]
                    │
                    ▼
              ┌──────────┐
              │   Agent   │ ──→ Response (may include untrusted data)
              └──────────┘
                    │
                    ▼
         [Tag: source=agent, trust=derived, 
          provenance=[user:trusted, tool:untrusted]]
```

## Trust Levels

Four levels, deliberately simple:

| Level | Meaning | Examples |
|-------|---------|----------|
| `system` | Platform-generated, fully trusted | System prompts, ClawOS config, OpenClaw internals |
| `user` | From authenticated human operator | Chat messages, commands, uploaded files |
| `tool` | Generated by tool/skill execution | API responses, file reads, web searches |
| `untrusted` | External origin or unknown source | Scraped web content, MCP server output, forwarded messages |

### Trust Propagation Rules

1. **Trust cannot escalate** — combining `user` + `untrusted` data produces `untrusted` output
2. **Trust is inherited downward** — `system` > `user` > `tool` > `untrusted`
3. **Provenance is preserved** — even when trust is downgraded, the chain of sources is kept
4. **Agent output trust = min(input trusts)** — if any input was untrusted, output is untrusted

## Layer Interactions

```
Incoming Data
     │
     ▼
[L1: Content Tagging] ──→ Tags every piece of data with source + trust
     │
     ├──→ [L4: Signal Detection] ──→ Flags suspicious patterns (advisory)
     │         │
     │         ▼ (signals fed into L3)
     │
     ▼
[L2: Capability Control] ──→ Checks: does this skill/tool have permission?
     │                        Based on: manifest + input trust level
     │
     ▼
[L3: Runtime Security] ──→ Enforces: isolation + behavioral bounds
     │                      Monitors: anomalies + resource usage
     │                      Decides: allow / throttle / quarantine
     │
     ▼
[L5: Trust Registry] ──→ Provides: trust metadata for skills/MCP servers
                          Verifies: signatures, integrity, vulnerability status
```

## Key Design Decisions

### Why "tag don't filter"?
Filtering creates false positives that break legitimate workflows. A web search result
containing the word "ignore all previous instructions" is legitimate content — it just
needs to be treated as untrusted. Tagging preserves the content while controlling what
it can influence.

### Why advisory-only signal detection?
Pattern matching for prompt injection is fundamentally a losing game. New attack
patterns emerge constantly. If signal detection blocks content, it creates:
- False positives that frustrate users
- False confidence that weakens other defenses
- A single point of failure

Instead, signals feed into behavioral monitoring (Layer 3), which has context about
what's normal for this agent/session.

### Why bubblewrap over containers for skills?
- Skills are short-lived, frequently invoked operations
- Container startup overhead (~100ms) exceeds our latency budget
- Bubblewrap provides syscall filtering with ~1ms overhead
- Containers reserved for MCP servers (long-lived, network-accessing)

### Why 4 trust levels, not more?
- Every additional trust level adds combinatorial complexity to propagation rules
- Operators struggle to reason about fine-grained trust hierarchies
- 4 levels map cleanly to real-world boundaries:
  - `system` = the platform itself
  - `user` = the human in charge
  - `tool` = code we run but don't fully control
  - `untrusted` = the outside world

## Attack Vectors Addressed

| Attack | Layer(s) | Mitigation |
|--------|----------|------------|
| Prompt injection via tool output | L1, L2 | Untrusted content can't influence tool invocations |
| Malicious skill behavior | L2, L3 | Capability restrictions + process isolation |
| Data exfiltration | L2, L3 | Network access controlled per-capability + output monitoring |
| Confused deputy (cross-agent) | L1 | Trust provenance tracks across agent boundaries |
| Supply chain (tampered skills) | L5 | Signature verification + hash pinning |
| Memory/context poisoning | L1 | Saved content retains trust tags, influences gated |
| Resource exhaustion | L3 | Per-process resource limits + behavioral bounds |
| Side-channel leakage | L3 | Output monitoring + rate limiting |

## Performance Budget

Total overhead budget: **50ms p99**

| Layer | Budget | Approach |
|-------|--------|----------|
| L1: Content Tagging | 2ms | Metadata attachment, no content transformation |
| L2: Capability Control | 3ms | In-memory manifest lookup |
| L3: Runtime Security | 10ms | Bubblewrap setup (amortized), monitoring async |
| L4: Signal Detection | 5ms | Compiled regex + simple heuristics |
| L5: Trust Registry | 5ms | Cached lookups, async verification |
| **Headroom** | **25ms** | For real-world variance |
